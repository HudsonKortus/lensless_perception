{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HudsonKortus/lensless_perception/blob/master/unet_chromatic_aberration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH6e4WmTNdEF",
        "outputId": "ed5dbe16-57c3-4a02-d57a-5d2a05986e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.11\n",
            "Torch : 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU 0: Tesla T4 (UUID: GPU-ec2f5508-7cfb-15ab-2d59-155d438084ce)\n",
            "Thu Oct  2 14:24:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch, platform\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch :\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# Small libs (usually already there on Colab)\n",
        "!pip -q install pillow imageio tqdm\n",
        "!nvidia-smi -L || echo \"No dedicated GPU visible\"\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxjTCf7pNiKD",
        "outputId": "5c1d5ffc-b260-47a2-8a82-78ab6fd6994c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HC4v9AyMQ3NP"
      },
      "outputs": [],
      "source": [
        "# Remove any previously extracted dataset\n",
        "!rm -rf /content/data\n",
        "\n",
        "# Unzip your archive.zip into /content/data\n",
        "!unzip -q \"/content/drive/MyDrive/dataset/archive.zip\" -d /content/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NrD9B09yOsfq"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!unzip -q \"/content/drive/MyDrive/dataset/archive.zip\" -d /content/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVuewYEhPDoI",
        "outputId": "b25d53a1-590b-44e9-8e64-ef5847c8689f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved /content/unet_depth_defocus.py\n"
          ]
        }
      ],
      "source": [
        "import os, math, argparse, numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import imageio.v2 as imageio\n",
        "import time\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------\n",
        "# Utils\n",
        "# ----------------------------\n",
        "\n",
        "def to_tensor_img(pil_img):\n",
        "    arr = np.array(pil_img, dtype=np.uint8)\n",
        "    return torch.from_numpy(arr).permute(2,0,1).float() / 255.0\n",
        "\n",
        "def save_depth_png(depth_m: np.ndarray,\n",
        "                   out_png: str,\n",
        "                   mode: str = \"fixed\",\n",
        "                   vmin: float = 0.3,\n",
        "                   vmax: float = 10.0,\n",
        "                   lo_p: float = 1.0,\n",
        "                   hi_p: float = 99.0):\n",
        "    \"\"\"\n",
        "    Save a depth map as an 8-bit PNG for visualization.\n",
        "\n",
        "    mode = \"fixed\": clamp to [vmin, vmax] meters, then normalize.\n",
        "    mode = \"auto\" : compute percentiles [lo_p, hi_p] per-image, then normalize.\n",
        "    \"\"\"\n",
        "    d = depth_m.astype(np.float32).copy()\n",
        "    if mode == \"fixed\":\n",
        "        d = np.clip(d, vmin, vmax)\n",
        "        d = (d - vmin) / (vmax - vmin + 1e-8)\n",
        "    elif mode == \"auto\":\n",
        "        lo, hi = np.percentile(d, [lo_p, hi_p])\n",
        "        if hi <= lo:  # fallback if degenerate\n",
        "            d[:] = 0.0\n",
        "        else:\n",
        "            d = np.clip((d - lo) / (hi - lo), 0.0, 1.0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown viz mode: {mode}\")\n",
        "    d8 = (d * 255.0).astype(np.uint8)\n",
        "    imageio.imwrite(out_png, d8)\n",
        "\n",
        "def read_depth_png_auto(p: Path):\n",
        "    im = Image.open(p)\n",
        "    arr = np.array(im)\n",
        "    arr = arr.astype(np.float32)\n",
        "    if arr.dtype == np.uint16 or arr.max() > 50.0:\n",
        "        arr = arr / 1000.0  # mm -> m\n",
        "    return arr\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset\n",
        "# ----------------------------\n",
        "#TODO add batch normalizaion\n",
        "#TODO add some more randomizaiton augmentaion just use the opencv command\n",
        "\n",
        "class NYUNativeTrain(Dataset):\n",
        "    \"\"\"\n",
        "    root/nyu2_train/<scene>/*.jpg and matching *.png (same stem) for depth\n",
        "    \"\"\"\n",
        "    def __init__(self, root, resize_hw=(240,320), center_crop=True, jitter=True):\n",
        "        self.root = Path(root)# / \"nyu2_train\"\n",
        "        self.resize_hw = resize_hw\n",
        "        self.center_crop = center_crop\n",
        "        self.jitter = jitter\n",
        "\n",
        "        self.pairs = []\n",
        "        scenes = [d for d in self.root.glob(\"*\") if d.is_dir()]\n",
        "        for s in tqdm(scenes, desc=\"Scan train scenes\"):\n",
        "            jpgs = sorted(s.glob(\"*.jpg\"))\n",
        "            for rgbp in jpgs:\n",
        "                dep = rgbp.with_suffix(\".png\")\n",
        "                if dep.exists():\n",
        "                    self.pairs.append((rgbp, dep))\n",
        "\n",
        "        # standard NYU crop box on 480x640\n",
        "        self.crop_box = (41,45,601,471)\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def _apply_crop(self, rgb_pil, depth_np):\n",
        "        l,t,r,b = self.crop_box\n",
        "        return rgb_pil.crop((l,t,r,b)), depth_np[t:b, l:r]\n",
        "\n",
        "    def _color_jitter(self, rgb_t):\n",
        "        if np.random.rand() < 0.5:\n",
        "            s = 0.9 + 0.2*np.random.rand(); rgb_t = torch.clamp(rgb_t*s, 0, 1)\n",
        "        if np.random.rand() < 0.5:\n",
        "            m = (np.random.rand(3)*0.1 - 0.05); rgb_t = torch.clamp(rgb_t + torch.tensor(m)[:,None,None], 0,1)\n",
        "        return rgb_t\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgbp, depp = self.pairs[idx]\n",
        "        rgb = Image.open(rgbp).convert(\"RGB\")\n",
        "        depth = read_depth_png_auto(depp)\n",
        "\n",
        "        if self.center_crop:\n",
        "            rgb, depth = self._apply_crop(rgb, depth)\n",
        "\n",
        "        H,W = self.resize_hw\n",
        "        rgb = rgb.resize((W,H), Image.BILINEAR)\n",
        "        depth = np.array(Image.fromarray(depth).resize((W,H), Image.NEAREST), dtype=np.float32)\n",
        "        rgb_t = to_tensor_img(rgb)\n",
        "        depth_t = torch.from_numpy(depth).float().clamp(0.3,10.0)\n",
        "\n",
        "        if self.jitter:\n",
        "            rgb_t = self._color_jitter(rgb_t)\n",
        "\n",
        "        return rgb_t, depth_t, rgbp.name\n",
        "\n",
        "class NYUNativeTest(Dataset):\n",
        "    \"\"\"\n",
        "    root/nyu2_test/*_colors.png & *_depth.png\n",
        "    \"\"\"\n",
        "    def __init__(self, root, resize_hw=(240,320), center_crop=True):\n",
        "        self.root = Path(root) / \"nyu2_test\"\n",
        "        self.resize_hw = resize_hw\n",
        "        self.center_crop = center_crop\n",
        "        self.files = []\n",
        "\n",
        "        color_files = sorted(self.root.glob(\"*_colors.png\"))\n",
        "        for c in color_files:\n",
        "            d = c.with_name(c.stem.replace(\"_colors\",\"_depth\") + c.suffix)\n",
        "            if d.exists():\n",
        "                self.files.append((c, d))\n",
        "\n",
        "        self.crop_box = (41,45,601,471)\n",
        "\n",
        "    def __len__(self): return len(self.files)\n",
        "\n",
        "    def _apply_crop(self, rgb_pil, depth_np):\n",
        "        l,t,r,b = self.crop_box\n",
        "        return rgb_pil.crop((l,t,r,b)), depth_np[t:b, l:r]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgbp, depp = self.files[idx]\n",
        "        rgb = Image.open(rgbp).convert(\"RGB\")\n",
        "        depth = read_depth_png_auto(depp)\n",
        "        if self.center_crop:\n",
        "            rgb, depth = self._apply_crop(rgb, depth)\n",
        "        H,W = self.resize_hw\n",
        "        rgb = rgb.resize((W,H), Image.BILINEAR)\n",
        "        depth = np.array(Image.fromarray(depth).resize((W,H), Image.NEAREST), dtype=np.float32)\n",
        "        rgb_t = to_tensor_img(rgb)\n",
        "        depth_t = torch.from_numpy(depth).float().clamp(0.3,10.0)\n",
        "        return rgb_t, depth_t, rgbp.stem  # stem like \"00000_colors\"\n",
        "\n",
        "# ----------------------------\n",
        "# Defocus simulator\n",
        "# ----------------------------\n",
        "\n",
        "def make_invdepth_edges(min_m=0.5, max_m=10.0, J=12):\n",
        "    inv = torch.linspace(1/max_m, 1/min_m, J+1)\n",
        "    return (1.0 / inv).float()\n",
        "\n",
        "def depth_to_soft_masks(depth_m, edges):\n",
        "    B,H,W = depth_m.shape; J = edges.numel()-1\n",
        "    masks = torch.zeros(B, J, 1, H, W, device=depth_m.device, dtype=depth_m.dtype)\n",
        "    for j in range(J):\n",
        "        m = (depth_m>=edges[j]) & (depth_m<edges[j+1])\n",
        "        masks[:,j,0] = m.float()\n",
        "    masks = F.avg_pool3d(masks, kernel_size=(1,3,3), stride=1, padding=(0,1,1))\n",
        "    masks = masks / (masks.sum(dim=1, keepdim=True) + 1e-8)\n",
        "    return masks\n",
        "\n",
        "def disk_kernel(radius_px, device, dtype=torch.float32, eps=1e-6):\n",
        "    r = max(0.4, float(radius_px))\n",
        "    R = int(math.ceil(r)) + 2\n",
        "    y = torch.arange(-R, R+1, device=device, dtype=torch.int32)\n",
        "    x = torch.arange(-R, R+1, device=device, dtype=torch.int32)\n",
        "    y, x = torch.meshgrid(y, x, indexing='ij')\n",
        "    k = ((x.to(torch.float32)**2 + y.to(torch.float32)**2) <= r*r + eps).to(dtype)\n",
        "    return k / (k.sum() + 1e-8)\n",
        "\n",
        "def chroma_radius(depth_m, ch_idx, fR=0.9, fG=1.0, fB=1.15, scale=120.0):\n",
        "    f = [fR,fG,fB][ch_idx]\n",
        "    return 1.0 + scale * abs(1.0/depth_m - 1.0/f)\n",
        "\n",
        "class ChromaticPSFBank(nn.Module):\n",
        "    def __init__(self, edges, fR=0.9, fG=1.0, fB=1.15, scale=120.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"edges\", edges.float())\n",
        "        self.J = self.edges.numel()-1\n",
        "        self.fR, self.fG, self.fB, self.scale = fR, fG, fB, scale\n",
        "        self._built = False\n",
        "\n",
        "    def _build(self, device, dtype=torch.float32):\n",
        "        self.kernels = []\n",
        "        for j in range(self.J):\n",
        "            zj = 2.0 / (1.0/self.edges[j] + 1.0/self.edges[j+1])  # harmonic mean depth\n",
        "            per_ch = []\n",
        "            for ch in range(3):\n",
        "                rad = chroma_radius(float(zj), ch, self.fR, self.fG, self.fB, self.scale)\n",
        "                per_ch.append(disk_kernel(rad, device, dtype=dtype))\n",
        "            self.kernels.append(per_ch)\n",
        "        self._built = True\n",
        "\n",
        "    def forward(self, rgb, masks):\n",
        "        rgb   = rgb.float()\n",
        "        masks = masks.float()\n",
        "        need_build = (\n",
        "            (not self._built) or\n",
        "            (self.kernels[0][0].device != rgb.device) or\n",
        "            (self.kernels[0][0].dtype  != rgb.dtype)\n",
        "        )\n",
        "        if need_build:\n",
        "            self._build(rgb.device, dtype=rgb.dtype)\n",
        "\n",
        "        out = torch.zeros_like(rgb, dtype=rgb.dtype)\n",
        "        for j in range(self.J):\n",
        "            Mj = masks[:, j, 0:1].to(dtype=rgb.dtype)\n",
        "            ch_imgs = []\n",
        "            for ch in range(3):\n",
        "                k = self.kernels[j][ch].to(device=rgb.device, dtype=rgb.dtype)[None, None]\n",
        "                b = F.conv2d(rgb[:, ch:ch+1], k, padding=k.shape[-1]//2)\n",
        "                ch_imgs.append(b)\n",
        "            blurred = torch.cat(ch_imgs, dim=1)\n",
        "            out += blurred * Mj\n",
        "        return torch.clamp(out, 0.0, 1.0)\n",
        "\n",
        "# ----------------------------\n",
        "# U-Net (compact)\n",
        "# ----------------------------\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, i, o):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(i,o,3,padding=1), nn.BatchNorm2d(o), nn.ReLU(True),\n",
        "            nn.Conv2d(o,o,3,padding=1), nn.BatchNorm2d(o), nn.ReLU(True),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, ch=[64,128,256,512,1024]):\n",
        "        super().__init__()\n",
        "        self.d1 = DoubleConv(3, ch[0]);  self.p1 = nn.MaxPool2d(2)\n",
        "        self.d2 = DoubleConv(ch[0], ch[1]);  self.p2 = nn.MaxPool2d(2)\n",
        "        self.d3 = DoubleConv(ch[1], ch[2]);  self.p3 = nn.MaxPool2d(2)\n",
        "        self.d4 = DoubleConv(ch[2], ch[3]);  self.p4 = nn.MaxPool2d(2)\n",
        "        self.b  = DoubleConv(ch[3], ch[4])\n",
        "\n",
        "        self.u4  = nn.ConvTranspose2d(ch[4], ch[3], 2, 2)\n",
        "        self.u4d = DoubleConv(ch[4], ch[3])\n",
        "\n",
        "        self.u3  = nn.ConvTranspose2d(ch[3], ch[2], 2, 2)\n",
        "        self.u3d = DoubleConv(ch[3], ch[2])\n",
        "\n",
        "        self.u2  = nn.ConvTranspose2d(ch[2], ch[1], 2, 2)\n",
        "        self.u2d = DoubleConv(ch[2], ch[1])\n",
        "\n",
        "        self.u1  = nn.ConvTranspose2d(ch[1], ch[0], 2, 2)\n",
        "        self.u1d = DoubleConv(ch[0] + ch[0], ch[0])\n",
        "\n",
        "        self.out = nn.Conv2d(ch[0], 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.d1(x)\n",
        "        d2 = self.d2(self.p1(d1))\n",
        "        d3 = self.d3(self.p2(d2))\n",
        "        d4 = self.d4(self.p3(d3))\n",
        "        b  = self.b(self.p4(d4))\n",
        "\n",
        "        x = self.u4(b); x = self.u4d(torch.cat([x, d4], 1))\n",
        "        x = self.u3(x); x = self.u3d(torch.cat([x, d3], 1))\n",
        "        x = self.u2(x); x = self.u2d(torch.cat([x, d2], 1))\n",
        "        x = self.u1(x); x = self.u1d(torch.cat([x, d1], 1))\n",
        "        return self.out(x)  # log-depth\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics\n",
        "# ----------------------------\n",
        "\n",
        "def depth_metrics(pred, gt):\n",
        "    valid = gt > 0\n",
        "    pred = pred[valid]; \n",
        "    gt = gt[valid]\n",
        "    vmin, vmax = 0.3, 10.0\n",
        "    pred = torch.clamp(pred, vmin, vmax); gt = torch.clamp(gt, vmin, vmax)\n",
        "    abs_rel = torch.mean(torch.abs(pred-gt)/gt).item()\n",
        "    rmse    = torch.sqrt(torch.mean((pred-gt)**2)).item()\n",
        "    ratio = torch.max(pred/gt, gt/pred)\n",
        "    d1 = (ratio < 1.25).float().mean().item()\n",
        "    return dict(abs_rel=abs_rel, rmse=rmse, delta1=d1)\n",
        "\n",
        "# ----------------------------\n",
        "# Train / Infer\n",
        "# ----------------------------\n",
        "\n",
        "def save_checkpoint(path, net, edges, sim_params, img_size):\n",
        "    ckpt = {\n",
        "        \"model\": net.state_dict(),\n",
        "        \"edges\": edges.detach().cpu(),\n",
        "        \"sim_params\": sim_params,\n",
        "        \"img_size\": img_size,\n",
        "    }\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "def train(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    H, W = args.height, args.width\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    train_ds = NYUNativeTrain(args.root, resize_hw=(H,W), center_crop=True, jitter=True)\n",
        "    val_ds   = NYUNativeTest(args.root,  resize_hw=(H,W), center_crop=True)\n",
        "\n",
        "    pin = torch.cuda.is_available()\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,\n",
        "                              num_workers=args.workers, drop_last=True,\n",
        "                              pin_memory=pin, persistent_workers=False)\n",
        "    \n",
        "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=pin, persistent_workers=False)\n",
        "    print(f\"[info] train samples={len(train_ds)} val samples={len(val_ds)}\")\n",
        "\n",
        "    edges = make_invdepth_edges(0.5, 10.0, args.num_bins).to(device)\n",
        "    simulator = ChromaticPSFBank(edges, fR=args.fR, fG=args.fG, fB=args.fB, scale=args.blur_scale).to(device)\n",
        "    net = UNet().to(device)\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler() if (args.amp and torch.cuda.is_available()) else None\n",
        "\n",
        "    pbar = tqdm(range(args.iters), desc=\"train iters\")\n",
        "    it_train = iter(train_loader)\n",
        "\n",
        "    _rgb, _depth, _ = next(it_train)\n",
        "    print(f\"[info] first batch ready: rgb {_rgb.shape}, depth {_depth.shape}\")\n",
        "\n",
        "    load_times, sim_times, net_times = [], [], []\n",
        "    sim_params = dict(fR=args.fR, fG=args.fG, fB=args.fB, scale=args.blur_scale)\n",
        "\n",
        "    for it in pbar:\n",
        "        start_load = time.time()\n",
        "        try:\n",
        "            rgb, depth, _ = next(it_train)\n",
        "        except StopIteration:\n",
        "            it_train = iter(train_loader)\n",
        "            rgb, depth, _ = next(it_train)\n",
        "        load_times.append(time.time() - start_load)\n",
        "\n",
        "        rgb = rgb.to(device).float()\n",
        "        depth = depth.to(device).float().clamp(0.3, 10.0)\n",
        "\n",
        "        start_sim = time.time()\n",
        "        masks = depth_to_soft_masks(depth, edges)\n",
        "        sim_rgb = simulator(rgb, masks)\n",
        "        sim_times.append(time.time() - start_sim)\n",
        "\n",
        "        start_net = time.time()\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred_logd = net(sim_rgb)\n",
        "                \n",
        "                gt_logd = torch.log(depth.unsqueeze(1) + 1e-6)\n",
        "                loss = F.mse_loss(pred_logd, gt_logd)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            pred_logd = net(sim_rgb)\n",
        "            gt_logd = torch.log(depth.unsqueeze(1) + 1e-6)\n",
        "            loss = F.mse_loss(pred_logd, gt_logd)\n",
        "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "        net_times.append(time.time() - start_net)\n",
        "\n",
        "        pbar.set_postfix(loss=float(loss.item()))\n",
        "\n",
        "        if (it+1) % 10 == 0:\n",
        "            avg_load = np.mean(load_times)\n",
        "            avg_sim  = np.mean(sim_times)\n",
        "            avg_net  = np.mean(net_times)\n",
        "            est_total = (avg_load+avg_sim+avg_net) * args.iters / 60.0\n",
        "            print(f\"[timing] avg load={avg_load:.3f}s, sim={avg_sim:.3f}s, net={avg_net:.3f}s -> est total ≈ {est_total:.1f} min\")\n",
        "\n",
        "        if args.save_every and (it+1) % args.save_every == 0:\n",
        "            ckpt_path = os.path.join(args.out_dir, f\"iter_{it+1}.pth\")\n",
        "            save_checkpoint(ckpt_path, net, edges, sim_params, (H,W))\n",
        "            print(f\"[ckpt] saved {ckpt_path}\")\n",
        "\n",
        "        if args.dry_run and (it+1) >= args.dry_run:\n",
        "            print(f\"[dry_run] Completed {args.dry_run} iterations. Exiting.\")\n",
        "            break\n",
        "\n",
        "    ckpt_path = os.path.join(args.out_dir, \"last.pth\")\n",
        "    save_checkpoint(ckpt_path, net, edges, sim_params, (H,W))\n",
        "    print(f\"[ckpt] saved final {ckpt_path}\")\n",
        "\n",
        "def infer_test(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    ckpt = torch.load(args.checkpoint, map_location=device)\n",
        "    net = UNet().to(device); net.load_state_dict(ckpt[\"model\"]); net.eval()\n",
        "    edges = ckpt.get(\"edges\", make_invdepth_edges()).to(device)\n",
        "    sim_params = ckpt.get(\"sim_params\", dict(fR=0.9,fG=1.0,fB=1.15,scale=120.0))\n",
        "    simulator = ChromaticPSFBank(edges, **sim_params).to(device)\n",
        "\n",
        "    H, W = ckpt.get(\"img_size\", (args.height, args.width))\n",
        "    test_ds = NYUNativeTest(args.root, resize_hw=(H,W), center_crop=True)\n",
        "\n",
        "    pin = torch.cuda.is_available()\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=1, shuffle=False,\n",
        "        num_workers=args.workers, pin_memory=pin, persistent_workers=False\n",
        "    )\n",
        "    print(f\"[info] test samples={len(test_ds)} workers={args.workers} pin_memory={pin}\")\n",
        "\n",
        "    # Select output dirs\n",
        "    out_dir_fixed = args.out_dir\n",
        "    out_dir_auto  = args.out_dir_auto if args.out_dir_auto else args.out_dir\n",
        "    if args.viz_mode == \"fixed\":\n",
        "        os.makedirs(out_dir_fixed, exist_ok=True)\n",
        "    elif args.viz_mode == \"auto\":\n",
        "        os.makedirs(out_dir_auto, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rgb, depth, stem in tqdm(test_loader, desc=\"infer_test\"):\n",
        "            rgb    = rgb.to(device).float()\n",
        "            ddepth = depth.to(device).float()\n",
        "            masks  = depth_to_soft_masks(ddepth, edges)\n",
        "            sim_rgb = simulator(rgb, masks)\n",
        "            pred_logd = net(sim_rgb)\n",
        "            pred_m = torch.exp(pred_logd).squeeze(0).squeeze(0).cpu().numpy()  # meters\n",
        "\n",
        "            base = stem[0].replace(\"_colors\",\"\")\n",
        "\n",
        "            # Save visualization(s)\n",
        "            if args.viz_mode == \"fixed\":\n",
        "                save_depth_png(pred_m, str(Path(out_dir_fixed)/f\"{base}_pred_depth.png\"),\n",
        "                               mode=\"fixed\", vmin=args.vmin, vmax=args.vmax)\n",
        "            elif args.viz_mode == \"auto\":\n",
        "                save_depth_png(pred_m, str(Path(out_dir_auto)/f\"{base}_pred_depth.png\"),\n",
        "                               mode=\"auto\", lo_p=args.viz_lo, hi_p=args.viz_hi)\n",
        "\n",
        "            # Optional: meters as .npy\n",
        "            if args.save_meters:\n",
        "                np.save(str(Path(out_dir_auto if args.viz_mode==\"auto\" else out_dir_fixed)/f\"{base}_pred_depth.npy\"),\n",
        "                        pred_m)\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "\n",
        "def get_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    sub = ap.add_subparsers(dest=\"cmd\", required=True)\n",
        "\n",
        "    tr = sub.add_parser(\"train\")\n",
        "    tr.add_argument(\"--root\", required=True, help=\"Folder that contains nyu2_train and nyu2_test\")\n",
        "    tr.add_argument(\"--out_dir\", required=True)\n",
        "    tr.add_argument(\"--iters\", type=int, default=20000)\n",
        "    tr.add_argument(\"--batch_size\", type=int, default=3)\n",
        "    tr.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    tr.add_argument(\"--height\", type=int, default=240)\n",
        "    tr.add_argument(\"--width\", type=int, default=320)\n",
        "    tr.add_argument(\"--num_bins\", type=int, default=12)\n",
        "    tr.add_argument(\"--fR\", type=float, default=0.9)\n",
        "    tr.add_argument(\"--fG\", type=float, default=1.0)\n",
        "    tr.add_argument(\"--fB\", type=float, default=1.15)\n",
        "    tr.add_argument(\"--blur_scale\", type=float, default=120.0)\n",
        "    tr.add_argument(\"--val_every\", type=int, default=2000)\n",
        "    tr.add_argument(\"--workers\", type=int, default=0, help=\"DataLoader workers (0 is safest on Windows)\")\n",
        "    tr.add_argument(\"--dry_run\", type=int, default=0, help=\"Run this many iterations then exit (debug)\")\n",
        "    tr.add_argument(\"--save_every\", type=int, default=1000, help=\"Save checkpoint every N iters (0=off)\")\n",
        "    tr.add_argument(\"--amp\", action=\"store_true\", help=\"Use mixed precision (CUDA AMP)\")\n",
        "\n",
        "    inf = sub.add_parser(\"infer_test\")\n",
        "    inf.add_argument(\"--root\", required=True)\n",
        "    inf.add_argument(\"--checkpoint\", required=True)\n",
        "    inf.add_argument(\"--out_dir\", required=True, help=\"Output directory for PNGs\")\n",
        "    # viz controls\n",
        "    inf.add_argument(\"--viz_mode\", choices=[\"fixed\",\"auto\"], default=\"fixed\",\n",
        "                     help=\"fixed: clamp to [vmin,vmax]; auto: per-image percentile stretching\")\n",
        "    inf.add_argument(\"--vmin\", type=float, default=0.3)\n",
        "    inf.add_argument(\"--vmax\", type=float, default=10.0)\n",
        "    inf.add_argument(\"--viz_lo\", type=float, default=1.0, help=\"auto-contrast low percentile\")\n",
        "    inf.add_argument(\"--viz_hi\", type=float, default=99.0, help=\"auto-contrast high percentile\")\n",
        "    inf.add_argument(\"--out_dir_auto\", type=str, default=\"\", help=\"Optional separate dir for auto viz\")\n",
        "    inf.add_argument(\"--save_meters\", action=\"store_true\", help=\"Also save .npy depth in meters\")\n",
        "    # shape & perf\n",
        "    inf.add_argument(\"--height\", type=int, default=240)\n",
        "    inf.add_argument(\"--width\", type=int, default=320)\n",
        "    inf.add_argument(\"--workers\", type=int, default=0)\n",
        "\n",
        "    return ap.parse_args()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    if args.cmd == \"train\":\n",
        "        train(args)\n",
        "    elif args.cmd == \"infer_test\":\n",
        "        infer_test(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REjT0Tb0P2kA",
        "outputId": "480a2f34-521a-4c0a-eb9a-dc679a678103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/data/nyu_data/data/nyu2_test\n",
            "/content/data/nyu_data/data/nyu2_train\n",
            "Scan train scenes: 100% 284/284 [00:01<00:00, 235.27it/s]\n",
            "[info] train samples=50688 val samples=654\n",
            "train iters:   0% 0/200 [00:00<?, ?it/s][info] first batch ready: rgb torch.Size([4, 3, 240, 320]), depth torch.Size([4, 240, 320])\n",
            "train iters:   4% 9/200 [00:11<02:57,  1.08it/s, loss=0.581][timing] avg load=0.000s, sim=0.129s, net=0.170s -> est total ≈ 1.0 min\n",
            "train iters:  10% 19/200 [00:20<02:42,  1.11it/s, loss=0.145][timing] avg load=0.000s, sim=0.069s, net=0.095s -> est total ≈ 0.5 min\n",
            "train iters:  14% 29/200 [00:29<02:35,  1.10it/s, loss=0.0369][timing] avg load=0.000s, sim=0.048s, net=0.069s -> est total ≈ 0.4 min\n",
            "train iters:  20% 39/200 [00:38<02:28,  1.08it/s, loss=2.92]  [timing] avg load=0.000s, sim=0.037s, net=0.056s -> est total ≈ 0.3 min\n",
            "train iters:  24% 49/200 [00:47<02:21,  1.07it/s, loss=0.0218][timing] avg load=0.000s, sim=0.031s, net=0.049s -> est total ≈ 0.3 min\n",
            "train iters:  30% 59/200 [00:57<02:13,  1.05it/s, loss=0.00503][timing] avg load=0.000s, sim=0.027s, net=0.044s -> est total ≈ 0.2 min\n",
            "train iters:  34% 69/200 [01:06<02:06,  1.04it/s, loss=0.00452][timing] avg load=0.000s, sim=0.024s, net=0.040s -> est total ≈ 0.2 min\n",
            "train iters:  40% 79/200 [01:16<01:58,  1.02it/s, loss=0.116][timing] avg load=0.000s, sim=0.022s, net=0.037s -> est total ≈ 0.2 min\n",
            "train iters:  44% 89/200 [01:26<01:51,  1.00s/it, loss=0.0275][timing] avg load=0.000s, sim=0.020s, net=0.035s -> est total ≈ 0.2 min\n",
            "train iters:  50% 99/200 [01:36<01:43,  1.02s/it, loss=0.0155][timing] avg load=0.000s, sim=0.019s, net=0.034s -> est total ≈ 0.2 min\n",
            "train iters:  55% 109/200 [01:47<01:33,  1.03s/it, loss=0.0092][timing] avg load=0.000s, sim=0.018s, net=0.032s -> est total ≈ 0.2 min\n",
            "train iters:  60% 119/200 [01:57<01:21,  1.01s/it, loss=0.0162][timing] avg load=0.000s, sim=0.017s, net=0.031s -> est total ≈ 0.2 min\n",
            "train iters:  64% 129/200 [02:07<01:10,  1.00it/s, loss=0.00193][timing] avg load=0.000s, sim=0.016s, net=0.030s -> est total ≈ 0.2 min\n",
            "train iters:  70% 139/200 [02:17<01:00,  1.01it/s, loss=0.000967][timing] avg load=0.000s, sim=0.016s, net=0.029s -> est total ≈ 0.2 min\n",
            "train iters:  74% 149/200 [02:27<00:50,  1.01it/s, loss=0.0058] [timing] avg load=0.000s, sim=0.015s, net=0.029s -> est total ≈ 0.1 min\n",
            "train iters:  80% 159/200 [02:36<00:40,  1.01it/s, loss=0.00334][timing] avg load=0.000s, sim=0.015s, net=0.028s -> est total ≈ 0.1 min\n",
            "train iters:  84% 169/200 [02:46<00:30,  1.00it/s, loss=0.0204][timing] avg load=0.000s, sim=0.014s, net=0.028s -> est total ≈ 0.1 min\n",
            "train iters:  90% 179/200 [02:56<00:21,  1.00s/it, loss=0.00214][timing] avg load=0.000s, sim=0.014s, net=0.027s -> est total ≈ 0.1 min\n",
            "train iters:  94% 189/200 [03:06<00:11,  1.00s/it, loss=0.00355][timing] avg load=0.000s, sim=0.013s, net=0.026s -> est total ≈ 0.1 min\n",
            "train iters: 100% 199/200 [03:16<00:01,  1.00s/it, loss=3.06]    [timing] avg load=0.000s, sim=0.013s, net=0.026s -> est total ≈ 0.1 min\n",
            "train iters: 100% 200/200 [03:16<00:00,  1.02it/s, loss=3.06]\n",
            "[ckpt] saved final /content/drive/MyDrive/nyu_runs/run1/last.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Verify where nyu2_train/nyu2_test actually are\n",
        "!find /content/data -maxdepth 3 -type d -name \"nyu2_*\" | sort\n",
        "\n",
        "# If you see them at /content/data/nyu_data/data/..., then set:\n",
        "ROOT = \"/content/data/nyu_data/data\"\n",
        "OUT  = \"/content/drive/MyDrive/nyu_runs/run1\"\n",
        "\n",
        "# 2) IMPORTANT: use $ROOT in the shell, not {ROOT}\n",
        "!python /content/unet_depth_defocus.py train \\\n",
        "  --root \"$ROOT\" \\\n",
        "  --out_dir \"$OUT\" \\\n",
        "  --iters 200 \\\n",
        "  --batch_size 4 \\\n",
        "  --lr 1e-3 \\\n",
        "  --height 240 \\\n",
        "  --width 320 \\\n",
        "  --num_bins 12 \\\n",
        "  --workers 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyDvTg_YVcti",
        "outputId": "f0734655-fec3-4df9-c126-2cc33eb77fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] test samples=654 workers=2 pin_memory=True\n",
            "infer_test: 100% 654/654 [02:15<00:00,  4.81it/s]\n"
          ]
        }
      ],
      "source": [
        "ROOT = \"/content/data/nyu_data/data\"\n",
        "CKPT = \"/content/drive/MyDrive/nyu_runs/run1/last.pth\"\n",
        "OUT_TEST = \"/content/drive/MyDrive/nyu_runs/run1/test_preds\"\n",
        "\n",
        "!python /content/unet_depth_defocus.py infer_test \\\n",
        "  --root \"$ROOT\" \\\n",
        "  --checkpoint \"$CKPT\" \\\n",
        "  --out_dir \"$OUT_TEST\" \\\n",
        "  --height 240 \\\n",
        "  --width 320 \\\n",
        "  --workers 2\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPYGDXrmsP4BQGC2fEt7Eb5",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
